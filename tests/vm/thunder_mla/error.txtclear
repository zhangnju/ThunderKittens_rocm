========= COMPUTE-SANITIZER
waiting on out_arrived
starting load wait for iteration 0, stage 0
finished load wait for iteration 0, stage 0
starting consumer wait for iteration 0, stage 0
finished consumer wait for iteration 0, stage 0
finished load launch for iteration 0, stage 0
finished loadfinished waiting on out_arrived
finished storer
Failed to import __get_quality__ from mla_decode.py
 ----------- starting seq_lengths: [128] new_tokens: 1 q_heads: 16 block_size: 128 -----------
Number of instructions: 1
o_scratch: tensor([[[[ 0.8074,  1.5243,  1.3019,  ..., -1.7114,  1.1237,  2.2109],
          [-1.4729,  1.2867,  2.0857,  ..., -1.9498, -0.1268, -0.1173],
          [-0.0638,  0.9561, -1.7078,  ..., -0.4704, -0.4864,  0.1340],
          ...,
          [-0.9333, -0.4878,  0.5811,  ..., -0.2531, -1.6977, -0.7119],
          [-1.6269,  0.5503,  0.1155,  ...,  1.1662, -0.4602, -1.4061],
          [-1.3373, -0.5310, -0.3013,  ...,  0.0781, -1.5570, -0.6846]]],


        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          ...,
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],


        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          ...,
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0')
lvec_scratch: tensor([[[ 1.0619,  0.5294, -1.1197, -0.1630,  1.5493,  0.6387,  0.6857,
           0.4447, -0.2499,  0.8524, -0.1670,  0.8792,  0.5754, -0.3779,
          -0.8088, -0.4495]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000,  0.0000]]], device='cuda:0')
Starting mla_decode
Time taken to run mla_decode: 0.08987307548522949 seconds
Finished mla_decode
[2]
torch.Size([3, 1, 16, 512]) torch.Size([3, 1, 16])
Finished!
torch.Size([1, 1, 16, 512]) torch.Size([1, 1, 16, 512])
torch.Size([1, 1, 16, 512]) torch.Size([1, 1, 16, 512])
torch.Size([1, 1, 16]) torch.Size([1, 1, 16])
O: tensor([[[[ 0.5469,  1.0312,  0.8789,  ..., -1.1562,  0.7617,  1.4922],
          [-0.8711,  0.7617,  1.2344,  ..., -1.1484, -0.0747, -0.0693],
          [-0.0201,  0.3008, -0.5391,  ..., -0.1484, -0.1533,  0.0422],
          ...,
          [-0.2490,  0.0796,  0.2266,  ..., -0.1099, -0.7383, -0.3105],
          [-0.6055,  0.2441, -0.1562,  ...,  0.4238, -0.1670, -0.5117],
          [-0.7070,  0.0255, -0.0972,  ...,  0.0330, -0.6602, -0.2891]]]],
       device='cuda:0', dtype=torch.bfloat16)
O_correct: tensor([[[[ 0.5469,  1.0312,  0.8789,  ..., -1.1562,  0.7617,  1.4922],
          [-0.8711,  0.7617,  1.2344,  ..., -1.1484, -0.0747, -0.0693],
          [-0.0201,  0.3008, -0.5391,  ..., -0.1484, -0.1533,  0.0422],
          ...,
          [-0.4062, -0.2119,  0.2520,  ..., -0.1099, -0.7383, -0.3105],
          [-0.5898,  0.2002,  0.0420,  ...,  0.4238, -0.1670, -0.5117],
          [-0.5664, -0.2246, -0.1270,  ...,  0.0330, -0.6602, -0.2891]]]],
       device='cuda:0', dtype=torch.bfloat16)
O - O_correct: tensor([-0.1406,  0.2500,  0.0298,  0.0015,  0.4668, -0.0977, -0.2402,  0.2051,
        -0.1172,  0.2461,  0.3574,  0.0195, -0.1504,  0.1016, -0.2949, -0.0986,
         0.0233, -0.0664, -0.1221,  0.4355, -0.0312,  0.0000, -0.1055,  0.0898,
        -0.2891,  0.2793,  0.5742,  0.1504,  0.3203, -0.1836,  0.0225,  0.0312,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       device='cuda:0', dtype=torch.bfloat16)
O max diff: tensor(1.0156, device='cuda:0', dtype=torch.bfloat16)
O mean diff: tensor(0.0074, device='cuda:0', dtype=torch.bfloat16)
O_scratch max diff: tensor(0., device='cuda:0')
O_scratch mean diff: tensor(0., device='cuda:0')
Lvec_scratch max diff: tensor(0., device='cuda:0')
Lvec_scratch mean diff: tensor(0., device='cuda:0')
========= ERROR SUMMARY: 0 errors
